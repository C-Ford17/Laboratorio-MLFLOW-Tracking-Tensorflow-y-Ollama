{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c014faf-e46f-48ed-948d-2b7caec1f0d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Paso 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2057454b-96ab-41f5-b670-bf0a453f18d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Dimensiones del dataset: (1055, 42)\n",
      "\n",
      "🔹 Columnas:\n",
      "['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'Class']\n",
      "\n",
      "🔹 Primeras filas:\n",
      "      V1      V2  V3  V4  V5  V6  V7    V8  V9  V10  ...  V33  V34  V35  \\\n",
      "0  3.919  2.6909   0   0   0   0   0  31.4   2    0  ...    0    0    0   \n",
      "1  4.170  2.1144   0   0   0   0   0  30.8   1    1  ...    0    0    0   \n",
      "2  3.932  3.2512   0   0   0   0   0  26.7   2    4  ...    0    0    1   \n",
      "3  3.000  2.7098   0   0   0   0   0  20.0   0    2  ...    0    0    1   \n",
      "4  4.236  3.3944   0   0   0   0   0  29.4   2    4  ...    0    0    0   \n",
      "\n",
      "     V36    V37  V38    V39  V40  V41  Class  \n",
      "0  2.949  1.591    0  7.253    0    0      2  \n",
      "1  3.315  1.967    0  7.257    0    0      2  \n",
      "2  3.076  2.417    0  7.601    0    0      2  \n",
      "3  3.046  5.000    0  6.690    0    0      2  \n",
      "4  3.351  2.405    0  8.003    0    0      2  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "\n",
      "🔹 Valores únicos de la variable objetivo 'class':\n",
      "Class\n",
      "1    699\n",
      "2    356\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# qsar_preprocessing.py\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Cargar el dataset desde OpenML\n",
    "qsar = fetch_openml(name=\"qsar-biodeg\", version=1, as_frame=True)\n",
    "df = qsar.frame\n",
    "\n",
    "# Información básica\n",
    "print(\"🔹 Dimensiones del dataset:\", df.shape)\n",
    "print(\"\\n🔹 Columnas:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\n🔹 Primeras filas:\")\n",
    "print(df.head())\n",
    "\n",
    "# Revisar la variable objetivo\n",
    "print(\"\\n🔹 Valores únicos de la variable objetivo 'class':\")\n",
    "print(df['Class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5bd127-4ab6-4105-a334-a5d5f5d379ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '1']\n",
      "Categories (2, object): ['1', '2']\n"
     ]
    }
   ],
   "source": [
    "print(df['Class'].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ddcb84-89c1-4335-95a1-98468243b28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "1    699\n",
      "0    356\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Asegurar tipo numérico y renombrar por claridad\n",
    "df['Class'] = df['Class'].astype(int)\n",
    "\n",
    "# Convertir a 0 y 1 (opcional, pero útil para modelos)\n",
    "df['Class'] = df['Class'].map({2: 0, 1: 1})\n",
    "\n",
    "print(df['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee94df99-465f-4c34-9980-fd0c42d2dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores únicos en la columna objetivo:\n",
      "[0 1]\n",
      "\n",
      "Cantidad de valores nulos en 'Class':\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Valores únicos en la columna objetivo:\")\n",
    "print(df['Class'].unique())\n",
    "\n",
    "print(\"\\nCantidad de valores nulos en 'Class':\")\n",
    "print(df['Class'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed4c60a0-082d-44cd-960a-a8acdd393c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Tamaños:\n",
      "Entrenamiento: (844, 41)  | Prueba: (211, 41)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n🔹 Tamaños:\")\n",
    "print(\"Entrenamiento:\", X_train.shape, \" | Prueba:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6fba20c-e982-43e3-8d07-7f81778468fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f2ae7-1dcf-40b1-a5e2-6f0645268ef0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Paso 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a149a3-abf0-41ed-9ff4-9064643388a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn  # Para modelos de Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb4010ed-5d50-4770-989c-ce3dee477233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/Christian/Documents/mlflow%202/mlruns/468532580829688563', creation_time=1760587998401, experiment_id='468532580829688563', last_update_time=1760587998401, lifecycle_stage='active', name='QSAR_Biodegradation_LogisticRegression', tags={'mlflow.experimentKind': 'custom_model_development'}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre de tu experimento\n",
    "experiment_name = \"QSAR_Biodegradation_LogisticRegression\"\n",
    "\n",
    "# Crear o seleccionar el experimento\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01123c83-18cf-46df-9374-e46a473c6375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run iniciado. Ahora se pueden registrar parámetros y métricas.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    # Aquí dentro iría el entrenamiento de tu modelo\n",
    "    mlflow.log_param(\"modelo\", \"Regresion Logistica\")\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "    \n",
    "    # Ejemplo: después de entrenar el modelo\n",
    "    # mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    print(\"Run iniciado. Ahora se pueden registrar parámetros y métricas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c2c89-eb3e-4a10-85fb-9edbc0f1088e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Paso 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3544d31-500c-4a28-9768-8e5864714405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/15 23:35:16 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/15 23:35:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Run completado. Métricas registradas en MLflow\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Importar librerías necesarias\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# 🔹 Configurar experimento\n",
    "mlflow.set_experiment(\"QSAR_Biodegradation_LogisticRegression\")\n",
    "\n",
    "# 🔹 Iniciar un run en MLflow\n",
    "with mlflow.start_run(run_name=\"LogisticRegression_Manual\"):\n",
    "\n",
    "    # Registrar parámetros que vas a probar\n",
    "    C = 1.0\n",
    "    max_iter = 200\n",
    "    solver = 'liblinear'  # bueno para datasets pequeños y clasificación binaria\n",
    "    mlflow.log_param(\"C\", C)\n",
    "    mlflow.log_param(\"max_iter\", max_iter)\n",
    "    mlflow.log_param(\"solver\", solver)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model = LogisticRegression(C=C, max_iter=max_iter, solver=solver)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predecir en el set de prueba\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Calcular métricas\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Registrar métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"precision\", prec)\n",
    "    mlflow.log_metric(\"recall\", rec)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Registrar tags\n",
    "    mlflow.set_tags({\n",
    "        \"framework\": \"scikit-learn\",\n",
    "        \"modelo\": \"Regresion Logistica\",\n",
    "        \"proposito\": \"Clasificación binaria QSAR Biodegradation\"\n",
    "    })\n",
    "\n",
    "    # Crear y guardar gráfico de matriz de confusión\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\n",
    "    plt.title(\"Matriz de Confusión\")\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Registrar artefactos (gráfico)\n",
    "    mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "\n",
    "    # Registrar descripción en texto\n",
    "    mlflow.log_text(\"Modelo entrenado con LogisticRegression de Scikit-learn sobre QSAR Biodegradation\", \"descripcion.txt\")\n",
    "\n",
    "    # Registrar modelo\n",
    "    mlflow.sklearn.log_model(model, \"modelo_logistico\")\n",
    "\n",
    "    print(\"✅ Run completado. Métricas registradas en MLflow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced0cf0-be36-4399-b18f-e297b90b3ea4",
   "metadata": {},
   "source": [
    "## Paso 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2298f073-e818-4623-a838-adf6575b2de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian\\Documents\\mlflow 2\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025/10/15 23:35:20 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m20/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4947 - loss: 0.7238 - precision: 0.6511 - recall: 0.5417"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6374 - loss: 0.6324 - precision: 0.7037 - recall: 0.7818 - val_accuracy: 0.7299 - val_loss: 0.5145 - val_precision: 0.7293 - val_recall: 0.9429\n",
      "Epoch 2/20\n",
      "\u001b[1m20/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7852 - loss: 0.4861 - precision: 0.7855 - recall: 0.9351 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8021 - loss: 0.4561 - precision: 0.8121 - recall: 0.9123 - val_accuracy: 0.8199 - val_loss: 0.4489 - val_precision: 0.8592 - val_recall: 0.8714\n",
      "Epoch 3/20\n",
      "\u001b[1m21/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8055 - loss: 0.4230 - precision: 0.8595 - recall: 0.8461 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8294 - loss: 0.3964 - precision: 0.8766 - recall: 0.8640 - val_accuracy: 0.8294 - val_loss: 0.4172 - val_precision: 0.8714 - val_recall: 0.8714\n",
      "Epoch 4/20\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8494 - loss: 0.3482 - precision: 0.8681 - recall: 0.8969 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8472 - loss: 0.3597 - precision: 0.8785 - recall: 0.8927 - val_accuracy: 0.8341 - val_loss: 0.3964 - val_precision: 0.8723 - val_recall: 0.8786\n",
      "Epoch 5/20\n",
      "\u001b[1m26/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8505 - loss: 0.3356 - precision: 0.8814 - recall: 0.8990 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8531 - loss: 0.3397 - precision: 0.8809 - recall: 0.8998 - val_accuracy: 0.8436 - val_loss: 0.3925 - val_precision: 0.8741 - val_recall: 0.8929\n",
      "Epoch 6/20\n",
      "\u001b[1m25/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8758 - loss: 0.3264 - precision: 0.8927 - recall: 0.9214  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8780 - loss: 0.3269 - precision: 0.9057 - recall: 0.9106 - val_accuracy: 0.8531 - val_loss: 0.3858 - val_precision: 0.8759 - val_recall: 0.9071\n",
      "Epoch 7/20\n",
      "\u001b[1m24/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9034 - loss: 0.2717 - precision: 0.9208 - recall: 0.9391 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8780 - loss: 0.3098 - precision: 0.8958 - recall: 0.9231 - val_accuracy: 0.8483 - val_loss: 0.3854 - val_precision: 0.8699 - val_recall: 0.9071\n",
      "Epoch 8/20\n",
      "\u001b[1m25/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8687 - loss: 0.3020 - precision: 0.8859 - recall: 0.9162 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8780 - loss: 0.2972 - precision: 0.9043 - recall: 0.9123 - val_accuracy: 0.8389 - val_loss: 0.3824 - val_precision: 0.8581 - val_recall: 0.9071\n",
      "Epoch 9/20\n",
      "\u001b[1m24/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8853 - loss: 0.2817 - precision: 0.9156 - recall: 0.9136 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8803 - loss: 0.2866 - precision: 0.9046 - recall: 0.9159 - val_accuracy: 0.8389 - val_loss: 0.3737 - val_precision: 0.8533 - val_recall: 0.9143\n",
      "Epoch 10/20\n",
      "\u001b[1m25/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8852 - loss: 0.2991 - precision: 0.8911 - recall: 0.9367 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8981 - loss: 0.2756 - precision: 0.9186 - recall: 0.9284 - val_accuracy: 0.8483 - val_loss: 0.3691 - val_precision: 0.8600 - val_recall: 0.9214\n",
      "Epoch 11/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8886 - loss: 0.2797 - precision: 0.9029 - recall: 0.9320 - val_accuracy: 0.8436 - val_loss: 0.3781 - val_precision: 0.8591 - val_recall: 0.9143\n",
      "Epoch 12/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8803 - loss: 0.2734 - precision: 0.9046 - recall: 0.9159 - val_accuracy: 0.8483 - val_loss: 0.3748 - val_precision: 0.8553 - val_recall: 0.9286\n",
      "Epoch 13/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8922 - loss: 0.2682 - precision: 0.9091 - recall: 0.9302 - val_accuracy: 0.8483 - val_loss: 0.3750 - val_precision: 0.8600 - val_recall: 0.9214\n",
      "Epoch 14/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9028 - loss: 0.2573 - precision: 0.9206 - recall: 0.9338 - val_accuracy: 0.8436 - val_loss: 0.3770 - val_precision: 0.8543 - val_recall: 0.9214\n",
      "Epoch 15/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8922 - loss: 0.2568 - precision: 0.9120 - recall: 0.9267 - val_accuracy: 0.8436 - val_loss: 0.3823 - val_precision: 0.8591 - val_recall: 0.9143\n",
      "Epoch 16/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9052 - loss: 0.2509 - precision: 0.9165 - recall: 0.9428 - val_accuracy: 0.8436 - val_loss: 0.3873 - val_precision: 0.8639 - val_recall: 0.9071\n",
      "Epoch 17/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9017 - loss: 0.2463 - precision: 0.9175 - recall: 0.9356 - val_accuracy: 0.8531 - val_loss: 0.3734 - val_precision: 0.8658 - val_recall: 0.9214\n",
      "Epoch 18/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8981 - loss: 0.2514 - precision: 0.9057 - recall: 0.9445 - val_accuracy: 0.8626 - val_loss: 0.3771 - val_precision: 0.8725 - val_recall: 0.9286\n",
      "Epoch 19/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9040 - loss: 0.2349 - precision: 0.9299 - recall: 0.9249 - val_accuracy: 0.8531 - val_loss: 0.3801 - val_precision: 0.8658 - val_recall: 0.9214\n",
      "Epoch 20/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8969 - loss: 0.2327 - precision: 0.9155 - recall: 0.9302 - val_accuracy: 0.8626 - val_loss: 0.3804 - val_precision: 0.8725 - val_recall: 0.9286\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/15 23:35:29 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Entrenamiento completado. Revisa la UI de MLflow para métricas y artefactos.\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Importar librerías\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# 🔹 Activar autologging de TensorFlow/Keras\n",
    "mlflow.tensorflow.autolog()\n",
    "\n",
    "# 🔹 Definir experimento\n",
    "mlflow.set_experiment(\"QSAR_Biodegradation_TensorFlowNN\")\n",
    "\n",
    "# 🔹 Crear el modelo secuencial\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # salida binaria\n",
    "])\n",
    "\n",
    "# 🔹 Compilar modelo\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# 🔹 Entrenamiento dentro de MLflow\n",
    "with mlflow.start_run(run_name=\"RedNeuronal_TF\"):\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"✅ Entrenamiento completado. Revisa la UI de MLflow para métricas y artefactos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece2601-8fb4-4b45-aaf8-ad49322de7a6",
   "metadata": {},
   "source": [
    "## Paso 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdebbf82-b9db-4f7c-a75f-1d1684f4b775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             run_id  metrics.accuracy  metrics.precision  \\\n",
      "0  70a62eb889f949f3a8f1328d2c922e41          0.853081           0.881119   \n",
      "1  50047884af344c29a957b209258f84a6          0.853081           0.881119   \n",
      "2  c42de85b144e436bbd8d86afb3fb11a5          0.853081           0.881119   \n",
      "3  238e73c2f7a44764be5b4f833a1c85cc          0.853081           0.881119   \n",
      "4  f1cadd21cd64416da2beef109ec381f3               NaN                NaN   \n",
      "5  228a1bf0db754bb982f4fb9f0fb1da88               NaN                NaN   \n",
      "6  35f6efabb92d42328133ae719fca61b3               NaN                NaN   \n",
      "7  4d110f7ab22a4398ae96bad0d99c38c4               NaN                NaN   \n",
      "\n",
      "   metrics.recall  metrics.f1_score params.C params.max_iter params.solver  \n",
      "0             0.9          0.890459      1.0             200     liblinear  \n",
      "1             0.9          0.890459      1.0             200     liblinear  \n",
      "2             0.9          0.890459      1.0             200     liblinear  \n",
      "3             0.9          0.890459      1.0             200     liblinear  \n",
      "4             NaN               NaN     None            None          None  \n",
      "5             NaN               NaN     None            None          None  \n",
      "6             NaN               NaN     None            None          None  \n",
      "7             NaN               NaN     None            None          None  \n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Seleccionar el experimento por nombre\n",
    "experiment = mlflow.get_experiment_by_name(\"QSAR_Biodegradation_LogisticRegression\")\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Buscar todos los runs\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"\",  # Puedes filtrar por tags o nombre: e.g. \"tags.modelo = 'Regresion Logistica'\"\n",
    "    order_by=[\"metrics.accuracy DESC\"]  # Ordenar por accuracy descendente\n",
    ")\n",
    "\n",
    "# Mostrar las primeras filas con métricas y parámetros\n",
    "print(runs[['run_id', 'metrics.accuracy', 'metrics.precision', 'metrics.recall', 'metrics.f1_score', 'params.C', 'params.max_iter', 'params.solver']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7b73efd-132e-40f8-891e-0bcfc47ea940",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs.to_csv(\"comparacion_runs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a96db-9aeb-4457-ada7-cb88b836dc8f",
   "metadata": {},
   "source": [
    "## Paso 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d85ace73-25da-42b5-9ab1-c1b8630dc238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/15 23:40:17 INFO mlflow.tracking.fluent: Experiment with name 'QSAR_Biodegradation_Interpretacion' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Respuestas de Ollama registradas en MLflow\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Configurar el cliente Ollama local\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "# Modelo que descargaste\n",
    "modelo = \"llama3\"\n",
    "\n",
    "# Lista de preguntas sobre los modelos\n",
    "preguntas = [\n",
    "    \"¿Qué significa obtener un F1-score de 0.88?\",\n",
    "    \"¿Por qué una red neuronal podría tener mejor recall que una regresión logística?\",\n",
    "    \"¿Qué podría mejorar el rendimiento de los modelos en este dataset?\"\n",
    "]\n",
    "\n",
    "respuestas = []\n",
    "\n",
    "for p in preguntas:\n",
    "    response = client.chat.completions.create(\n",
    "        model=modelo,\n",
    "        messages=[{\"role\": \"user\", \"content\": p}]\n",
    "    )\n",
    "    r = response.choices[0].message.content.strip()\n",
    "    respuestas.append(f\"Pregunta: {p}\\nRespuesta: {r}\\n\")\n",
    "\n",
    "# Guardar en archivo\n",
    "archivo = \"interpretacion_ollama.txt\"\n",
    "with open(archivo, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(respuestas)\n",
    "\n",
    "# Registrar en MLflow\n",
    "mlflow.set_experiment(\"QSAR_Biodegradation_Interpretacion\")\n",
    "with mlflow.start_run(run_name=\"Interpretacion_Ollama_Notebook\"):\n",
    "    mlflow.log_artifact(archivo)\n",
    "    print(\"✅ Respuestas de Ollama registradas en MLflow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93319987-7968-4fa5-bb82-8988f4b52eec",
   "metadata": {},
   "source": [
    "## Paso 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb2de598-0545-474d-a9bc-779bf685c10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/15 23:41:07 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/10/15 23:41:08 INFO mlflow.tracking.fluent: Autologging successfully enabled for openai.\n",
      "2025/10/15 23:41:09 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# Activar autologging global\n",
    "mlflow.autolog()\n",
    "mlflow.sklearn.autolog()\n",
    "mlflow.tensorflow.autolog()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1a598b1-e27d-4562-8401-825959ec9bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/15 23:42:24 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n",
      "2025/10/15 23:42:27 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.8759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/15 23:42:30 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Accuracy: 0.8577\n",
      "Fold 3 Accuracy: 0.8683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "with mlflow.start_run(run_name=\"LogisticRegression_CV\") as parent_run:\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]  # <- CORRECTO\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"Fold_{fold+1}\", nested=True):\n",
    "            model = LogisticRegression(C=1.0, max_iter=200, solver='liblinear')\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            acc = model.score(X_val_fold, y_val_fold)\n",
    "            mlflow.log_metric(\"accuracy\", acc)\n",
    "            print(f\"Fold {fold+1} Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9ee0d-40d5-48a8-8963-d140edefd991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLflow (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
